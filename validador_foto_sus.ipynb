{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU1zo-RrsO3p",
        "outputId": "ba1e4c5f-19ed-4b14-cf73-87c611efd5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'face-parsing'...\n",
            "remote: Enumerating objects: 221, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 221 (delta 7), reused 52 (delta 5), pack-reused 161 (from 3)\u001b[K\n",
            "Receiving objects: 100% (221/221), 28.50 MiB | 14.42 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n",
            "/content/face-parsing\n"
          ]
        }
      ],
      "source": [
        "# face-parsing server para segmentar partes do rosto. segmenta 19 partes na respectiva ordem (com correspondência numérica):\n",
        "# 'background', 'skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',\n",
        "# 'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat'\n",
        "# obs: (1) vai ser utilizada para acusar identificar a quantidade de olhos, e o uso de óculos e/ou chapéu\n",
        "# (2) a classe mouth só retorna algo quando a boca está aberta, seja num sorriso seja num momento de surpresa.\n",
        "# essa classe será utilizada para ajudar a análise de emoções, uma vez que o deepface dá alguns falso positivos.\n",
        "# (3) não entendo que ela possa ser utilizada para detectar o uso de máscara facial. ela tem classes\n",
        "# relacionadas aos lábios superiores e inferiores, mas, por exemplo, dependendo da foto ela não consegue segmentar\n",
        "# os meus lábios por causa da barba\n",
        "\n",
        "!git clone https://github.com/yakhyo/face-parsing.git\n",
        "%cd face-parsing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vou usar os pesos resnet34.pt, eles são melhores para identificar óculos mais finos e com bordas invisíveis\n",
        "# coisa que a outro modelo (resnet18.pt) não foi capaz de fazer\n",
        "\n",
        "# arquivo de pesos do face-parsing\n",
        "url_pesos = \"https://github.com/yakhyo/face-parsing/releases/download/v0.0.1/resnet34.pt\"\n",
        "nome_do_arquivo_de_pesos = \"resnet34.pt\"\n",
        "\n",
        "!wget -O {nome_do_arquivo_de_pesos} {url_pesos}\n",
        "\n",
        "print(f\"\\n✅ Download concluído! O arquivo '{nome_do_arquivo_de_pesos}' está na pasta /content/face-parsing.\")\n",
        "\n",
        "weights_path = '/content/face-parsing/resnet34.pt'\n",
        "\n",
        "# URL do arquivo de modelos do deepface\n",
        "URL=\"https://github.com/deepinsight/insightface/releases/download/v0.7/antelopev2.zip\"\n",
        "\n",
        "# Cria o diretório de destino dos modelos\n",
        "!mkdir -p /root/.insightface/models\n",
        "\n",
        "# Baixa o arquivo ZIP para o diretório temporário\n",
        "!wget -O /tmp/antelopev2.zip $URL\n",
        "\n",
        "# Descompacta o arquivo para a pasta 'models' (um saco acertar qual era a pasta!)\n",
        "# A opção -o força a substituição sem perguntar.\n",
        "# O destino agora é 'models', não 'models/antelopev2'\n",
        "!unzip -o /tmp/antelopev2.zip -d /root/.insightface/models/\n",
        "\n",
        "print(f\"\\n✅ Download concluído! O arquivo antelopev2.zip foi extraído na pasta correta /root/.insightface/models/antelopev2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OujK1-nkskmP",
        "outputId": "e952e320-6ec1-456a-8b23-8f744d7a5c66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-02 21:05:51--  https://github.com/yakhyo/face-parsing/releases/download/v0.0.1/resnet34.pt\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/785633039/439df296-f4ef-49ed-961f-774b3b295412?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250702T210551Z&X-Amz-Expires=1800&X-Amz-Signature=5802686c3b25d9229f6d2c084d35f9605cb0e7f12ce1b37bf4a6fc58f4655b8e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dresnet34.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-02 21:05:51--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/785633039/439df296-f4ef-49ed-961f-774b3b295412?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250702T210551Z&X-Amz-Expires=1800&X-Amz-Signature=5802686c3b25d9229f6d2c084d35f9605cb0e7f12ce1b37bf4a6fc58f4655b8e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dresnet34.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95869966 (91M) [application/octet-stream]\n",
            "Saving to: ‘resnet34.pt’\n",
            "\n",
            "resnet34.pt         100%[===================>]  91.43M   116MB/s    in 0.8s    \n",
            "\n",
            "2025-07-02 21:05:52 (116 MB/s) - ‘resnet34.pt’ saved [95869966/95869966]\n",
            "\n",
            "\n",
            "✅ Download concluído! O arquivo 'resnet34.pt' está na pasta /content/face-parsing.\n",
            "--2025-07-02 21:05:53--  https://github.com/deepinsight/insightface/releases/download/v0.7/antelopev2.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/102057483/185dda5b-a7ee-40c1-9253-f50b956108de?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250702T210553Z&X-Amz-Expires=1800&X-Amz-Signature=9c0c18b6a227fe1b7cbd05e7c46dd9cbbbf069baa293263f25d1acfb880f4302&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dantelopev2.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-02 21:05:53--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/102057483/185dda5b-a7ee-40c1-9253-f50b956108de?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250702%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250702T210553Z&X-Amz-Expires=1800&X-Amz-Signature=9c0c18b6a227fe1b7cbd05e7c46dd9cbbbf069baa293263f25d1acfb880f4302&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dantelopev2.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 360662982 (344M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/antelopev2.zip’\n",
            "\n",
            "/tmp/antelopev2.zip 100%[===================>] 343.95M   124MB/s    in 2.8s    \n",
            "\n",
            "2025-07-02 21:05:56 (124 MB/s) - ‘/tmp/antelopev2.zip’ saved [360662982/360662982]\n",
            "\n",
            "Archive:  /tmp/antelopev2.zip\n",
            "   creating: /root/.insightface/models/antelopev2/\n",
            "  inflating: /root/.insightface/models/antelopev2/genderage.onnx  \n",
            "  inflating: /root/.insightface/models/antelopev2/2d106det.onnx  \n",
            "  inflating: /root/.insightface/models/antelopev2/1k3d68.onnx  \n",
            "  inflating: /root/.insightface/models/antelopev2/glintr100.onnx  \n",
            "  inflating: /root/.insightface/models/antelopev2/scrfd_10g_bnkps.onnx  \n",
            "\n",
            "✅ Download concluído! O arquivo antelopev2.zip foi extraído na pasta correta /root/.insightface/models/antelopev2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deepface = análise de emoções\n",
        "# rembg = remove o fundo\n",
        "# insightface = landmarks do rosto\n",
        "# onnxruntime = necessário para insightface\n",
        "# pillow-heif = abrir .heic\n",
        "\n",
        "!pip install -q onnxruntime insightface pillow-heif deepface rembg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnT8X68tslUo",
        "outputId": "36f24432-8297-4b62-e737-7b50a269861d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/439.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/439.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m430.1/439.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importa o que precisa\n",
        "import pillow_heif\n",
        "pillow_heif.register_heif_opener()\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import insightface\n",
        "from insightface.app import FaceAnalysis\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from scipy.ndimage import label, center_of_mass\n",
        "from rembg import remove\n",
        "from deepface import DeepFace\n",
        "import json\n",
        "\n",
        "# le as constantes\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "MODO_DEBUG       = config[\"MODO_DEBUG\"]\n",
        "DIMENSAO_INTERNA = config[\"DIMENSAO_INTERNA\"]\n",
        "AJUSTA_MARGEM    = config[\"AJUSTA_MARGEM\"]\n",
        "LIMIAR_EAR       = config[\"LIMIAR_EAR\"]\n",
        "LIMIAR_BLI       = config[\"LIMIAR_BLI\"]\n",
        "LIMIAR_FELIZ     = config[\"LIMIAR_FELIZ\"]\n",
        "LIMIAR_OCULOS    = config[\"LIMIAR_OCULOS\"]\n",
        "LIMIAR_CHAPEU    = config[\"LIMIAR_CHAPEU\"]\n",
        "LIMIAR_FRONT     = config[\"LIMIAR_FRONT\"]\n",
        "LIMIAR_ROSTO    = config[\"LIMIAR_ROSTO\"]\n",
        "\n",
        "\n",
        "# flags dos testes\n",
        "TESTE_LATERAL_CONTRAST = config[\"TESTE_LATERAL_CONTRAST\"]\n",
        "TESTE_FACE_FRONTAL     = config[\"TESTE_FACE_FRONTAL\"]\n",
        "TESTE_EYES_OPEN        = config[\"TESTE_EYES_OPEN\"]\n",
        "TESTE_BOTH_EYES        = config[\"TESTE_BOTH_EYES\"]\n",
        "TESTE_EXPRESSION       = config[\"TESTE_EXPRESSION\"]\n",
        "TESTE_GLASSES          = config[\"TESTE_GLASSES\"]\n",
        "TESTE_HAT              = config[\"TESTE_HAT\"]\n",
        "TESTE_FACE_MASK        = config[\"TESTE_FACE_MASK\"]\n",
        "\n",
        "\n",
        "# upload da foto e conversão caso necessário\n",
        "print(\"Faça upload da imagem:\")\n",
        "uploaded = files.upload()\n",
        "img_name = next(iter(uploaded))\n",
        "ext = os.path.splitext(img_name)[1].lower()\n",
        "\n",
        "# arquivos heic e heif não são lidos no deepface, precisam ser convertidos\n",
        "if ext in ['.heic', '.heif']:\n",
        "    img_pil = Image.open(img_name).convert('RGB')\n",
        "    base_no_ext = os.path.splitext(img_name)[0]\n",
        "    converted_name = base_no_ext + \".jpg\"\n",
        "    img_pil.save(converted_name, \"JPEG\")\n",
        "    if MODO_DEBUG:\n",
        "        print('Arquivo HEIC/HEIF convertido para JPG.')\n",
        "    os.remove(img_name)\n",
        "else:\n",
        "    converted_name = img_name\n",
        "    img_pil = Image.open(converted_name).convert('RGB')\n",
        "    base_no_ext = os.path.splitext(converted_name)[0]\n",
        "    if MODO_DEBUG:\n",
        "        print('Arquivo já está em formato compatível.')\n",
        "\n",
        "# reduz o tamanho das fotos\n",
        "def open_image_from_pil(img_pil, max_side):\n",
        "    img = np.array(img_pil)\n",
        "    if img.shape[-1] == 4:\n",
        "        img = img[:, :, :3]\n",
        "    ih, iw = img.shape[:2]\n",
        "    if max(ih, iw) > max_side:\n",
        "        scale = max_side / max(ih, iw)\n",
        "        img = cv2.resize(img, (int(iw * scale), int(ih * scale)))\n",
        "    if MODO_DEBUG:\n",
        "        print(f\"Imagem de tamanho: {img.shape}\")\n",
        "    return img\n",
        "\n",
        "img = open_image_from_pil(img_pil,2*DIMENSAO_INTERNA)\n",
        "img_pil_small = Image.fromarray(img)\n",
        "img_pil_small.save(converted_name)\n",
        "\n",
        "# inicia o insightface para detecção da face e landmarks\n",
        "face_app = FaceAnalysis(name=\"antelopev2\",\n",
        "                        providers=['CPUExecutionProvider'])\n",
        "face_app.prepare(ctx_id=-1, det_thresh=LIMIAR_ROSTO, det_size=(DIMENSAO_INTERNA, DIMENSAO_INTERNA))\n",
        "def detect_face_bbox_and_landmarks(img_rgb):\n",
        "    faces = face_app.get(img_rgb)\n",
        "    if not faces or not hasattr(faces[0], \"landmark_2d_106\"):\n",
        "        return None, None\n",
        "    main_face = faces[0]\n",
        "    x1, y1, x2, y2 = [int(a) for a in main_face.bbox]\n",
        "    face_landmarks = [(int(x), int(y)) for x, y in main_face.landmark_2d_106]\n",
        "    return (x1, y1, x2, y2), face_landmarks\n",
        "\n",
        "face_bbox, face_landmarks = detect_face_bbox_and_landmarks(img)\n",
        "\n",
        "if face_bbox is None or face_landmarks is None:\n",
        "    print(\"E01: Nenhum rosto detectado.\")\n",
        "    # Gera arquivo de resultado automático (erro de rosto)\n",
        "    resultado_json = { \"foto_valida\": False, \"erros\": [\"E01: Nenhum rosto detectado.\"], \"detalhes\": [] }\n",
        "    with open(\"results.json\", \"w\") as f:\n",
        "        json.dump(resultado_json, f, indent=2, ensure_ascii=False)\n",
        "else:\n",
        "    # Corta a foto em 3x4 em torno do rosto encontrado\n",
        "    def crop_face_3x4_return_bbox(img_rgb, bbox, margin):\n",
        "        ih, iw = img_rgb.shape[:2]\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        w = x2 - x1\n",
        "        h = y2 - y1\n",
        "        cx = x1 + w//2\n",
        "        cy = y1 + h//2\n",
        "        box_h = int(h * (1 + margin))\n",
        "        box_w = int(box_h * 3 / 4)\n",
        "        # Ajuste para não sair das bordas\n",
        "        nx1 = max(cx - box_w//2, 0)\n",
        "        nx2 = min(cx + box_w//2, iw)\n",
        "        ny1 = max(cy - box_h//2, 0)\n",
        "        ny2 = min(cy + box_h//2, ih)\n",
        "        crop = img_rgb[int(ny1):int(ny2), int(nx1):int(nx2)]\n",
        "        final = cv2.resize(crop, (480, 640))\n",
        "        return final, (int(nx1), int(ny1), int(nx2), int(ny2))\n",
        "\n",
        "    # Esse recorte é útil para reduzir a dimensão da imagem e ajudar as detecções\n",
        "    crop_3x4, (nx1, ny1, nx2, ny2) = crop_face_3x4_return_bbox(img, face_bbox, AJUSTA_MARGEM)\n",
        "    img = crop_3x4\n",
        "    img_pil_resized = Image.fromarray(img)\n",
        "    crop_filename = base_no_ext + \"_crop3x4.png\"\n",
        "    img_pil_resized.save(crop_filename)\n",
        "    #converted_name = crop_filename\n",
        "\n",
        "    if MODO_DEBUG:\n",
        "    # Mostra o crop 3x4 para debug\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Crop 3x4 para DEBUG\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    # FACE PARSING\n",
        "    from models.bisenet import BiSeNet\n",
        "    weights_path = '/content/face-parsing/resnet34.pt'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    backbone = 'resnet18' if '18' in os.path.basename(weights_path) else 'resnet34'\n",
        "    n_classes = 19\n",
        "    model = BiSeNet(n_classes, backbone)\n",
        "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    to_tensor = transforms.Compose([\n",
        "        transforms.Resize((DIMENSAO_INTERNA, DIMENSAO_INTERNA)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_tensor = to_tensor(img_pil_resized).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(img_tensor)[0]\n",
        "        parsing = out.squeeze(0).cpu().numpy()\n",
        "        parsing = np.argmax(parsing, axis=0)  # (512,512)\n",
        "\n",
        "    # faz o debug das máscaras de segmentação\n",
        "    # localiza as máscaras dos olhos e ajusta a detecção deles\n",
        "    colors = np.array([\n",
        "        [0, 0, 0], [204, 204, 255], [153, 0, 0], [255, 255, 102],\n",
        "        [0, 255, 255], [255, 0, 255], [102, 51, 0], [102, 0, 102],\n",
        "        [255, 153, 204], [0, 204, 153], [255, 255, 255], [255, 0, 0],\n",
        "        [153, 255, 153], [0, 0, 255], [0, 255, 0], [255, 255, 0],\n",
        "        [153, 204, 255], [255, 153, 153], [153, 102, 255]\n",
        "    ], dtype=np.uint8)\n",
        "    parsing_img = Image.fromarray(parsing.astype(np.uint8), mode='P')\n",
        "    parsing_img.putpalette(colors.flatten())\n",
        "    parsing_img_color = parsing_img.convert(\"RGBA\")\n",
        "    image_rgba = img_pil_resized.resize(parsing_img_color.size).convert(\"RGBA\")\n",
        "    blend_opacity = 120\n",
        "    parsing_arr = np.array(parsing_img_color)\n",
        "    mask_fg = parsing_arr[..., 0:3].sum(axis=-1) != 0\n",
        "    parsing_arr[..., 3] = 0\n",
        "    parsing_arr[mask_fg, 3] = blend_opacity\n",
        "    parsing_img_transp = Image.fromarray(parsing_arr, mode=\"RGBA\")\n",
        "    blended = Image.alpha_composite(image_rgba, parsing_img_transp)\n",
        "\n",
        "    all_eye_mask = ((parsing == 4) | (parsing == 5))\n",
        "    eye_labels, num_eyes = label(all_eye_mask)\n",
        "    eye_blobs = []\n",
        "    centers = []\n",
        "    for i in range(1, num_eyes + 1):\n",
        "        blob = (eye_labels == i)\n",
        "        area = blob.sum()\n",
        "        if area > 10:\n",
        "            cY, cX = center_of_mass(blob)\n",
        "            eye_blobs.append(blob)\n",
        "            centers.append((cX, cY))\n",
        "    blobs_img = np.zeros((parsing.shape[0], parsing.shape[1], 3), dtype=np.uint8)\n",
        "    for i, blob in enumerate(eye_blobs):\n",
        "        color = [0, 0, 0]\n",
        "        if i == 0:\n",
        "            color = [0, 255, 255]\n",
        "        elif i == 1:\n",
        "            color = [255, 0, 255]\n",
        "        blobs_img[blob] = color\n",
        "\n",
        "    # imprime as máscaras junto as com as imagens\n",
        "    if MODO_DEBUG:\n",
        "        plt.figure(figsize=(15,5))\n",
        "        plt.subplot(1,3,1)\n",
        "        plt.imshow(img_pil_resized.resize(parsing_img_color.size))\n",
        "        plt.title('Original')\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.imshow(img_pil_resized.resize(parsing_img_color.size))\n",
        "        plt.imshow(blobs_img, alpha=0.5)\n",
        "        plt.title(f'Olhos detectados: {len(eye_blobs)}')\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.imshow(blended)\n",
        "        plt.title('Segmentações')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # funções de detecção a seguir\n",
        "\n",
        "    # numeração dos landmarks\n",
        "    LEFT_EYE_CONTOUR = [35, 41, 40, 42, 39, 37, 33, 36]\n",
        "    LEFT_EYE_CENTER = 38\n",
        "    RIGHT_EYE_CONTOUR = [93, 96, 94, 95, 89, 90, 87, 91]\n",
        "    RIGHT_EYE_CENTER = 88\n",
        "    NOSE_TIP = 86\n",
        "\n",
        "    # função para verificar o constraste lateral\n",
        "    def lateral_contrast(face_gray, mask_face, min_diff):\n",
        "        h, w = face_gray.shape\n",
        "        center = w//2\n",
        "        l_mask = np.zeros_like(mask_face)\n",
        "        r_mask = np.zeros_like(mask_face)\n",
        "        l_mask[:, :center] = 1\n",
        "        r_mask[:, center:] = 1\n",
        "        l_face = face_gray[(mask_face==1) & (l_mask==1)]\n",
        "        r_face = face_gray[(mask_face==1) & (r_mask==1)]\n",
        "        mean_l = np.mean(l_face) if len(l_face)>0 else 0\n",
        "        mean_r = np.mean(r_face) if len(r_face)>0 else 0\n",
        "        abs_diff = abs(mean_l - mean_r)\n",
        "        maximum = max(mean_l,mean_r)\n",
        "        ratio = abs_diff/maximum\n",
        "        if maximum>0 and ratio>min_diff:\n",
        "            return False, f\"E02: Diferença de brilho entre os lados acima do limite: esq={mean_l:.1f}, dir={mean_r:.1f} (razao={ratio:.2f})\"\n",
        "        return True, f\"Iluminação lateral satisfatória: esq={mean_l:.1f}, dir={mean_r:.1f} (razao={ratio:.2f})\"\n",
        "\n",
        "    # função para verificar se o rosto está virado para frente\n",
        "    def is_face_frontal(landmarks,limiar):\n",
        "        left_eye = np.array(landmarks[LEFT_EYE_CENTER])\n",
        "        right_eye = np.array(landmarks[RIGHT_EYE_CENTER])\n",
        "        nose = np.array(landmarks[NOSE_TIP])\n",
        "        eye_center_x = (left_eye[0] + right_eye[0]) / 2\n",
        "        if abs(nose[0] - eye_center_x) > limiar:\n",
        "            return False, \"E03: Rosto não está completamente frontal\"\n",
        "        return True, \"Rosto orientado para frente\"\n",
        "\n",
        "    # função para verificar a razão de aspecto do olho, isto é, se ele está aberto ou fechado\n",
        "    def eye_aspect_ratio(landmarks, indices):\n",
        "        pts = [np.array(landmarks[i]) for i in indices]\n",
        "        A = np.linalg.norm(pts[1] - pts[5])\n",
        "        B = np.linalg.norm(pts[2] - pts[4])\n",
        "        C = np.linalg.norm(pts[0] - pts[3])\n",
        "        return (A + B) / (2.0 * C)\n",
        "\n",
        "    # verifica se os olhos estão abertos\n",
        "    def check_eyes_open_landmarks(landmarks, parsing, threshold):\n",
        "        left_indices = [35, 40, 42, 39, 37, 33]\n",
        "        right_indices = [93, 94, 95, 89, 90, 87]\n",
        "        left_EAR = eye_aspect_ratio(landmarks, left_indices)\n",
        "        right_EAR = eye_aspect_ratio(landmarks, right_indices)\n",
        "        bool_eyes, _ = both_eyes_detected(parsing)\n",
        "        bool_glasses, _ = has_glasses_by_parsing(parsing,LIMIAR_OCULOS)\n",
        "        if (left_EAR < threshold or right_EAR < threshold or (not bool_eyes and not bool_glasses)):\n",
        "            return False, f\"E04: Olhos aparentemente fechados ou não encontrados (EAR: {left_EAR:.2f}/{right_EAR:.2f})\"\n",
        "        elif bool_glasses:\n",
        "            return False, f\"E04: Olhos podem estar abertos, mas usando óculos (EAR: {left_EAR:.2f}/{right_EAR:.2f})\"\n",
        "        else:\n",
        "            return True, f\"Olhos abertos e sem óculos (EAR: {left_EAR:.2f}/{right_EAR:.2f})\"\n",
        "\n",
        "    # verifica se ambos os olhos são detectados, algum pode estar obstruído\n",
        "    def both_eyes_detected(parsing):\n",
        "        all_eye_mask = ((parsing == 4) | (parsing == 5))\n",
        "        eye_labels, num_eyes = label(all_eye_mask)\n",
        "        centers = []\n",
        "        for i in range(1, num_eyes + 1):\n",
        "            blob = (eye_labels == i)\n",
        "            area = blob.sum()\n",
        "            if area > 10:\n",
        "                cY, cX = center_of_mass(blob)\n",
        "                centers.append((cX, cY))\n",
        "        if len(centers) >= 2:\n",
        "            dist = abs(centers[1][0] - centers[0][0])\n",
        "            if dist > 30:\n",
        "                return True, \"Ambos os olhos visíveis pela segmentação\"\n",
        "            else:\n",
        "                return False, \"E05: Apenas um olho separado visível ou blobs grudados pela segmentação\"\n",
        "        elif len(centers) == 1:\n",
        "            return False, \"E05: Apenas um olho identificado (um dos olhos pode estar coberto)\"\n",
        "        else:\n",
        "            return False, \"E05: Nenhum olho identificado pela segmentação (olhos fechados ou usando óculos)\"\n",
        "\n",
        "    # verifica se a expressão é neutra\n",
        "    def is_neutral_expression(img_rgb, parsing, threshold):\n",
        "        try:\n",
        "            result = DeepFace.analyze(img_path=crop_filename, actions=['emotion'], enforce_detection=False)\n",
        "            emotion = result[0]['dominant_emotion']\n",
        "            conf_neutral = result[0]['emotion']['neutral']\n",
        "            conf_happy = result[0]['emotion'].get('happy', 0)\n",
        "            h_img, w_img = img_rgb.shape[:2]\n",
        "            parsing_resized = cv2.resize(parsing.astype(np.uint8), (w_img, h_img), interpolation=cv2.INTER_NEAREST)\n",
        "            mouth_mask = (parsing_resized == 11)\n",
        "            mouth_pixels = np.sum(mouth_mask)\n",
        "\n",
        "            if mouth_pixels > threshold:\n",
        "                return False, f\"E06: Possível sorriso detectado (boca aberta - {mouth_pixels} px > {threshold} px)\"\n",
        "            elif (emotion == 'neutral') or (emotion == 'sad') or (emotion == 'fear') or (emotion == 'angry') or (emotion == 'happy'):\n",
        "                return True, f\"Expressão aceita ({emotion} - {conf_neutral:.1f}% de neutralidade)\"\n",
        "            else:\n",
        "                return False, f\"E06: Expressão não aceita ({emotion} - {conf_neutral:.1f}% de neutralidade)\"\n",
        "        except Exception as e:\n",
        "            return False, f\"Erro ao detectar expressão: {e}\"\n",
        "\n",
        "    # verifica se está usando óculos\n",
        "    def has_glasses_by_parsing(parsing,limiar):\n",
        "        glasses_mask = (parsing == 6)\n",
        "        if np.sum(glasses_mask) > limiar:\n",
        "            return True, \"E07: Óculos detectados pela segmentação\"\n",
        "        return False, \"Nenhum par de óculos detectado\"\n",
        "\n",
        "    # verifica se está usando chapéu\n",
        "    def has_hat_by_parsing(parsing,limiar):\n",
        "        hat_mask = (parsing == 18)\n",
        "        if np.sum(hat_mask) > limiar:\n",
        "            return True, \"E08: Chapéu/boné detectadoa pela segmentação\"\n",
        "        return False, \"Nenhum chapéu detectado\"\n",
        "\n",
        "    # verifica se está usando chapéu\n",
        "    def has_face_mask_by_parsing(parsing):\n",
        "        lip_u_mask = (parsing == 12)\n",
        "        lip_d_mask = (parsing == 13)\n",
        "        lips_pixels = np.sum(lip_u_mask)+np.sum(lip_d_mask)\n",
        "        if lips_pixels == 0:\n",
        "            return True, \"E09: Boca obstruída: provável uso de máscara facial detectado pela segmentação\"\n",
        "        return False, \"Nenhuma obstrução na boca detectada\"\n",
        "\n",
        "\n",
        "    # pipeline principal\n",
        "    result_msgs = []\n",
        "    erros = []\n",
        "    ok_all = True\n",
        "\n",
        "    face_bbox_local, face_landmarks_local = detect_face_bbox_and_landmarks(img)\n",
        "    if (face_bbox_local is None) or (face_landmarks_local is None):\n",
        "        erro_ = \"E01: Nenhum rosto detectado no crop\"\n",
        "        result_msgs.append(erro_)\n",
        "        erros.append(erro_)\n",
        "        ok_all = False\n",
        "    else:\n",
        "        ih, iw = img.shape[:2]\n",
        "        img_gray = np.mean(img, axis=2).astype(np.uint8)\n",
        "        x1, y1, x2, y2 = face_bbox_local\n",
        "        face_region = img_gray[y1:y2, x1:x2]\n",
        "        mask_dummy_face = np.ones_like(face_region, dtype=np.uint8)\n",
        "\n",
        "        if TESTE_LATERAL_CONTRAST:\n",
        "            ok_lateral, msg_lateral = lateral_contrast(face_region, mask_dummy_face, LIMIAR_BLI)\n",
        "            result_msgs.append(msg_lateral)\n",
        "            if not ok_lateral:\n",
        "                ok_all = False\n",
        "                erros.append(msg_lateral)\n",
        "\n",
        "        if TESTE_FACE_FRONTAL:\n",
        "            ok2, msg2 = is_face_frontal(face_landmarks_local,LIMIAR_FRONT)\n",
        "            result_msgs.append(f\"{msg2}\")\n",
        "            if not ok2:\n",
        "                ok_all = False\n",
        "                erros.append(msg2)\n",
        "\n",
        "        if TESTE_EYES_OPEN:\n",
        "            ok3, msg3 = check_eyes_open_landmarks(face_landmarks_local, parsing, LIMIAR_EAR)\n",
        "            result_msgs.append(f\"{msg3}\")\n",
        "            if not ok3:\n",
        "                ok_all = False\n",
        "                erros.append(msg3)\n",
        "\n",
        "        if TESTE_BOTH_EYES:\n",
        "            ok_parsing_eye, msg_parsing_eye = both_eyes_detected(parsing)\n",
        "            result_msgs.append(f\"{msg_parsing_eye}\")\n",
        "            if not ok_parsing_eye:\n",
        "                ok_all = False\n",
        "                erros.append(msg_parsing_eye)\n",
        "\n",
        "        if TESTE_EXPRESSION:\n",
        "            neutral, msg_neutral = is_neutral_expression(img, parsing, LIMIAR_FELIZ)\n",
        "            result_msgs.append(f\"{msg_neutral}\")\n",
        "            if not neutral:\n",
        "                ok_all = False\n",
        "                erros.append(msg_neutral)\n",
        "\n",
        "        if TESTE_GLASSES:\n",
        "            glasses, msg_glasses = has_glasses_by_parsing(parsing,LIMIAR_OCULOS)\n",
        "            result_msgs.append(f\"{msg_glasses}\")\n",
        "            if glasses:\n",
        "                ok_all = False\n",
        "                erros.append(msg_glasses)\n",
        "\n",
        "        if TESTE_HAT:\n",
        "            hat, msg_hat = has_hat_by_parsing(parsing,LIMIAR_CHAPEU)\n",
        "            result_msgs.append(f\"{msg_hat}\")\n",
        "            if hat:\n",
        "                ok_all = False\n",
        "                erros.append(msg_hat)\n",
        "\n",
        "        if TESTE_FACE_MASK:\n",
        "            facemask, msg_facemask = has_face_mask_by_parsing(parsing)\n",
        "            result_msgs.append(f\"{msg_facemask}\")\n",
        "            if facemask:\n",
        "                ok_all = False\n",
        "                erros.append(msg_facemask)\n",
        "\n",
        "    #   PLOT, OUTPUT FINAL\n",
        "    if ok_all:\n",
        "        print(\"\\n FOTO VÁLIDA!\\n\")\n",
        "    else:\n",
        "        print(\"\\n FOTO INVÁLIDA!\\n\")\n",
        "    for msg in result_msgs:\n",
        "        print(msg)\n",
        "\n",
        "    # gera json com o resultado das validações\n",
        "    resultado_json = {\n",
        "        \"foto_valida\": ok_all,\n",
        "        \"erros\": erros,\n",
        "        \"detalhes\": result_msgs\n",
        "    }\n",
        "    with open(\"results.json\", \"w\") as f:\n",
        "        json.dump(resultado_json, f, indent=2, ensure_ascii=False)\n",
        "    if MODO_DEBUG:\n",
        "        print(\"Relatório salvo em results.json\")\n",
        "\n",
        "    if face_bbox_local is not None and face_landmarks_local is not None:\n",
        "\n",
        "        if MODO_DEBUG:\n",
        "            img_show = cv2.cvtColor(img.copy(), cv2.COLOR_RGB2BGR)\n",
        "            color = (0,255,0) if ok_all else (0,0,255)\n",
        "            cv2.rectangle(img_show, (face_bbox_local[0],face_bbox_local[1]), (face_bbox_local[2],face_bbox_local[3]), color, 3)\n",
        "            for idx in LEFT_EYE_CONTOUR: cv2.circle(img_show, face_landmarks_local[idx], 2, (0,0,255), -1)\n",
        "            cv2.circle(img_show, face_landmarks_local[LEFT_EYE_CENTER], 2, (0,255,0), -1)\n",
        "            for idx in RIGHT_EYE_CONTOUR: cv2.circle(img_show, face_landmarks_local[idx], 2, (255,0,0), -1)\n",
        "            cv2.circle(img_show, face_landmarks_local[RIGHT_EYE_CENTER], 2, (0,255,0), -1)\n",
        "            cv2.circle(img_show, face_landmarks_local[NOSE_TIP], 3, (0,255,255), -1)\n",
        "            plt.figure(figsize=(7,7))\n",
        "            plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
        "            plt.title(\"Face detectada\" if ok_all else \"Problema na validação\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "        if ok_all:\n",
        "            # Aqui, já está no crop 3x4 pronto! Só usa rembg/fundo branco se quiser\n",
        "            keep_classes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n",
        "            crop_parsing = parsing\n",
        "            h_crop, w_crop = img.shape[:2]\n",
        "            crop_parsing_resized = cv2.resize(crop_parsing.astype(np.uint8), (w_crop, h_crop), interpolation=cv2.INTER_NEAREST)\n",
        "            mask_sem = np.isin(crop_parsing_resized, keep_classes).astype(np.uint8)\n",
        "            crop_pil = Image.fromarray(img)\n",
        "            crop_rmbg = remove(crop_pil)\n",
        "            arr_rmbg = np.array(crop_rmbg)\n",
        "            if arr_rmbg.shape[-1] == 4:\n",
        "                mask_rmbg = (arr_rmbg[:,:,3] > 128).astype(np.uint8)\n",
        "            else:\n",
        "                mask_rmbg = np.ones_like(mask_sem)\n",
        "            mask_rmbg = cv2.resize(mask_rmbg, (w_crop, h_crop), interpolation=cv2.INTER_NEAREST)\n",
        "            mask_final = np.clip(mask_rmbg + mask_sem, 0, 1)\n",
        "            mask_f = mask_final.astype(np.float32)\n",
        "            mask_f = cv2.GaussianBlur(mask_f, (15, 15), sigmaX=0)\n",
        "            mask_f = np.clip(mask_f, 0, 1)\n",
        "            if len(mask_f.shape) == 2:\n",
        "                mask3 = np.repeat(mask_f[:, :, np.newaxis], 3, axis=2)\n",
        "            else:\n",
        "                mask3 = mask_f\n",
        "            bg_branco = np.ones_like(img, dtype=np.float32)*255\n",
        "            cropf = img.astype(np.float32)\n",
        "            out_crop = cropf * mask3 + bg_branco * (1 - mask3)\n",
        "            out_crop = np.clip(out_crop, 0, 255).astype(np.uint8)\n",
        "            final_3x4 = cv2.resize(out_crop, (480, 640))\n",
        "            output_filename = base_no_ext + \"_cropped_final_3x4.png\"\n",
        "            pil_face = Image.fromarray(final_3x4)\n",
        "            pil_face.save(output_filename)\n",
        "\n",
        "            if MODO_DEBUG:\n",
        "                from IPython.display import display\n",
        "                display(pil_face)\n",
        "\n",
        "            print(f\"Recorte 3x4 salvo como {output_filename}\")\n",
        "            files.download(output_filename)\n",
        "\n",
        "    # limpeza dos arquivos\n",
        "    if os.path.exists(img_name):\n",
        "        os.remove(img_name)\n",
        "        if MODO_DEBUG:\n",
        "            print(f\"Arquivo inicial removido.\")\n",
        "\n",
        "    if ext in ['.heic', '.heif']:\n",
        "        if os.path.exists(converted_name) and (converted_name != img_name):\n",
        "            os.remove(converted_name)\n",
        "            if MODO_DEBUG:\n",
        "                print(f\"Arquivo convertido '{converted_name}' removido.\")\n",
        "\n",
        "    if os.path.exists(crop_filename):\n",
        "        os.remove(crop_filename)\n",
        "        if MODO_DEBUG:\n",
        "            print(f\"Arquivo crop '{crop_filename}' removido.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "C-TkCAZ5lC9W",
        "outputId": "6cbb1cb0-6c98-4ff7-d967-293c9d027dd7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faça upload da imagem:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-39ded43e-98fc-4546-b7e7-be0067f9505a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-39ded43e-98fc-4546-b7e7-be0067f9505a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving foto5.jpg to foto5.jpg\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/antelopev2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/antelopev2/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/antelopev2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/antelopev2/glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/antelopev2/scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "set det-size: (512, 512)\n",
            "\n",
            " FOTO INVÁLIDA!\n",
            "\n",
            "Iluminação lateral satisfatória: esq=120.3, dir=162.1 (razao=0.26)\n",
            "Rosto orientado para frente\n",
            "E04: Olhos podem estar abertos, mas usando óculos (EAR: 0.32/0.30)\n",
            "E05: Nenhum olho identificado pela segmentação (olhos fechados ou usando óculos)\n",
            "Expressão aceita (neutral - 55.2% de neutralidade)\n",
            "E07: Óculos detectados pela segmentação\n",
            "Nenhum chapéu detectado\n",
            "Nenhuma obstrução na boca detectada\n"
          ]
        }
      ]
    }
  ]
}
